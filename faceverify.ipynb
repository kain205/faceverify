{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\faceverify_id\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from dataclasses import dataclass\n",
    "import os  \n",
    "import winsound\n",
    "\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\faceverify_id\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:115: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: C:\\Users\\ADMIN/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: C:\\Users\\ADMIN/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ADMIN/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: C:\\Users\\ADMIN/.insightface\\models\\buffalo_l\\genderage.onnx genderage\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ADMIN/.insightface\\models\\buffalo_l\\model.onnx recognition ['None', 3, 112, 112] 0.0 1.0\n",
      "set det-size: (640, 640)\n",
      "=== Face Verification System ===\n",
      "Step 1: Detecting ID card automatically\n",
      "Step 2: Detecting live face automatically\n",
      "Step 3: Comparing faces...\n",
      "Restarting verification process\n",
      "Step 1: Detecting ID card automatically\n",
      "Step 2: Detecting live face automatically\n",
      "Step 3: Comparing faces...\n",
      "Exiting face verification system\n"
     ]
    }
   ],
   "source": [
    "# Constants and configuration\n",
    "@dataclass\n",
    "class IDCardSpecs:\n",
    "    aspect_ratio: float\n",
    "    template_features: dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_standard_id():\n",
    "        return IDCardSpecs(\n",
    "            aspect_ratio=1.58,  # Standard ID card aspect ratio\n",
    "            template_features={\n",
    "                \"logo\": (0.05, 0.05, 0.3, 0.2),     # x1, y1, x2, y2 in percentage\n",
    "                \"photo_area\": (0.7, 0.2, 0.95, 0.7), # x1, y1, x2, y2 in percentage\n",
    "            }\n",
    "        )\n",
    "def sound_play(sound_path):\n",
    "    \"\"\"Play a sound file using winsound\"\"\"\n",
    "    try:\n",
    "        winsound.PlaySound(sound_path, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "    except Exception as e:\n",
    "        print(f\"Error playing sound: {e}\")\n",
    "\n",
    "# Face detection functions\n",
    "def init_face_detector():\n",
    "    \"\"\"Initialize face detector with option for lightweight detection\"\"\"\n",
    "    # Instead of hardcoding file paths, check for file existence or use a more reliable method\n",
    "    try:\n",
    "        model_file = \"opencv_face_detector_uint8.pb\"\n",
    "        config_file = \"opencv_face_detector.pbtxt\"\n",
    "        detector = cv2.dnn.readNetFromTensorflow(model_file, config_file)\n",
    "        return {'type': 'opencv_dnn', 'model': detector}\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing face detector: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def detect_faces(detector, image, min_confidence=0.92, min_face_size=20):\n",
    "    \"\"\"Detect faces in an image with performance optimizations\"\"\"\n",
    "    # Resize image for faster processing (optional)\n",
    "    scale_factor = 1.0  # Reduce to 0.5 for even faster processing\n",
    "    if scale_factor != 1.0:\n",
    "        h, w = image.shape[:2]\n",
    "        image = cv2.resize(image, (int(w*scale_factor), int(h*scale_factor)))\n",
    "    \n",
    "    # OpenCV DNN-based detection\n",
    "    h, w = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    detector['model'].setInput(blob)\n",
    "    detections = detector['model'].forward()\n",
    "    \n",
    "    faces = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > min_confidence:\n",
    "            x1 = int(detections[0, 0, i, 3] * w)\n",
    "            y1 = int(detections[0, 0, i, 4] * h)\n",
    "            x2 = int(detections[0, 0, i, 5] * w)\n",
    "            y2 = int(detections[0, 0, i, 6] * h)\n",
    "            \n",
    "            # Convert to MTCNN-compatible format\n",
    "            faces.append({\n",
    "                'box': [x1, y1, x2-x1, y2-y1],\n",
    "                'confidence': float(confidence),\n",
    "                'keypoints': {}  # No keypoints with this detector\n",
    "            })\n",
    "    return faces\n",
    "\n",
    "def draw_face(image, faces):\n",
    "    \"\"\"Draw rectangle around detected face\"\"\"\n",
    "    if not faces:\n",
    "        return image\n",
    "    \n",
    "    display_image = image.copy()\n",
    "    x, y, w, h = faces[0]['box']\n",
    "    confidence = faces[0]['confidence']\n",
    "    cv2.rectangle(display_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    text = f\"{w} x {h}, conf: {confidence:.2f}\"\n",
    "    cv2.putText(display_image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    return display_image\n",
    "\n",
    "# Face comparison functions\n",
    "def init_face_analysis(det_size=(640, 640)):\n",
    "    \"\"\"Initialize face analysis with only necessary models for embedding extraction\"\"\"\n",
    "    app = FaceAnalysis(allowed_modules=['detection', 'recognition'])\n",
    "    app.prepare(ctx_id=0, det_size=det_size)\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image):\n",
    "    \"\"\"Extract face embedding from image\"\"\"\n",
    "    faces = app.get(image)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    if hasattr(face, 'det_score') and face.det_score < 0.5:\n",
    "        print(f\"Warning: Low quality face detection (score: {face.det_score:.2f})\")\n",
    "    \n",
    "    return face.embedding\n",
    "\n",
    "def compare_face_embeddings(feat1, feat2):\n",
    "    \"\"\"Compare two face embeddings and return similarity score\"\"\"\n",
    "    if feat1 is None or feat2 is None:\n",
    "        return -1.0  # Return negative value to indicate invalid comparison\n",
    "        \n",
    "    # Normalize vectors to unit length\n",
    "    feat1 = feat1 / np.linalg.norm(feat1)\n",
    "    feat2 = feat2 / np.linalg.norm(feat2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    sim = np.dot(feat1, feat2)\n",
    "    return sim\n",
    "\n",
    "def is_same_person(feat1, feat2, threshold=0.3):\n",
    "    \"\"\"Check if two face embeddings belong to the same person\"\"\"\n",
    "    if feat1 is None or feat2 is None:\n",
    "        return False\n",
    "    return compare_face_embeddings(feat1, feat2) > threshold\n",
    "\n",
    "# ID card detection functions\n",
    "def extract_reference_features(reference_image, card_specs):\n",
    "    \"\"\"Extract features from reference ID card image\"\"\"\n",
    "    # Convert to grayscale for feature extraction\n",
    "    gray = cv2.cvtColor(reference_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create(nfeatures=1000)\n",
    "    \n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "    \n",
    "    # Store reference aspect ratio and features\n",
    "    h, w = reference_image.shape[:2]\n",
    "    reference_features = {\n",
    "        \"keypoints\": keypoints,\n",
    "        \"descriptors\": descriptors,\n",
    "        \"aspect_ratio\": float(w) / h,\n",
    "        \"width\": w,\n",
    "        \"height\": h\n",
    "    }\n",
    "    \n",
    "    return reference_features\n",
    "\n",
    "def detect_id_card(frame, reference, reference_features, card_specs, min_matches=15):\n",
    "    \"\"\"\n",
    "    Detect ID card in frame using ORB feature matching with stability improvements\n",
    "    \"\"\"\n",
    "    # Convert frame to grayscale for feature detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply histogram equalization to improve feature detection\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    \n",
    "    # Initialize ORB detector with more features and better parameters\n",
    "    orb = cv2.ORB_create(\n",
    "        nfeatures=2000,          # Increase number of features\n",
    "        scaleFactor=1.2,         # Smaller scale factor for better multi-scale detection\n",
    "        nlevels=8,               # More scale levels\n",
    "        edgeThreshold=31,        # Avoid features at image borders\n",
    "        firstLevel=0,\n",
    "        WTA_K=2,\n",
    "        patchSize=31,            # Larger patch size for more distinctive features\n",
    "        fastThreshold=20         # Adjust FAST detector threshold\n",
    "    )\n",
    "    \n",
    "    # Detect keypoints and compute descriptors for current frame\n",
    "    keypoints_frame, descriptors_frame = orb.detectAndCompute(gray, None)\n",
    "    \n",
    "    # If no features found, return early\n",
    "    if descriptors_frame is None or len(keypoints_frame) < 8:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Create feature matcher - use KNN matcher instead of BFMatcher with crossCheck\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "    \n",
    "    # Match descriptors between reference and current frame using KNN\n",
    "    # This gives us the 2 best matches for each descriptor\n",
    "    matches = bf.knnMatch(reference_features[\"descriptors\"], descriptors_frame, k=2)\n",
    "    \n",
    "    # Apply ratio test to filter good matches (Lowe's ratio test)\n",
    "    good_matches = []\n",
    "    for match_pair in matches:\n",
    "        if len(match_pair) >= 2:\n",
    "            m, n = match_pair\n",
    "            if m.distance < 0.75 * n.distance:  # Ratio test\n",
    "                good_matches.append(m)\n",
    "    \n",
    "    # Check if we have enough good matches\n",
    "    if len(good_matches) < min_matches:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Extract matched keypoints\n",
    "    ref_pts = np.float32([reference_features[\"keypoints\"][m.queryIdx].pt for m in good_matches])\n",
    "    frame_pts = np.float32([keypoints_frame[m.trainIdx].pt for m in good_matches])\n",
    "    \n",
    "    # Find homography to map reference points to frame points\n",
    "    # Use more stringent RANSAC threshold\n",
    "    H, mask = cv2.findHomography(ref_pts, frame_pts, cv2.RANSAC, 3.0)\n",
    "    \n",
    "    if H is None:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Count inliers (matches that fit the homography model)\n",
    "    inlier_count = np.sum(mask)\n",
    "    \n",
    "    # Require a minimum number of inliers (stronger check)\n",
    "    if inlier_count < min_matches * 0.7:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Get dimensions of reference image\n",
    "    h, w = reference.shape[:2]\n",
    "    \n",
    "    # Define reference corners\n",
    "    ref_corners = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Project reference corners to frame using homography\n",
    "    frame_corners = cv2.perspectiveTransform(ref_corners, H)\n",
    "    \n",
    "    # Convert to integer points for drawing\n",
    "    corners = np.int32(frame_corners)\n",
    "    \n",
    "    # Check if the quadrilateral is convex\n",
    "    if not cv2.isContourConvex(corners):\n",
    "        return False, None, None\n",
    "    \n",
    "    # Check if detected quadrilateral has reasonable aspect ratio\n",
    "    detected_width = max(\n",
    "        np.linalg.norm(corners[0][0] - corners[3][0]),\n",
    "        np.linalg.norm(corners[1][0] - corners[2][0])\n",
    "    )\n",
    "    detected_height = max(\n",
    "        np.linalg.norm(corners[0][0] - corners[1][0]),\n",
    "        np.linalg.norm(corners[2][0] - corners[3][0])\n",
    "    )\n",
    "    \n",
    "    if detected_height < 50 or detected_width < 50:  # Card is too small\n",
    "        return False, None, None\n",
    "        \n",
    "    if detected_height == 0:\n",
    "        return False, None, None\n",
    "        \n",
    "    detected_aspect = detected_width / detected_height\n",
    "    \n",
    "    # Check if aspect ratio is close to expected card aspect ratio\n",
    "    aspect_tolerance = 0.2  # Tighter tolerance (was 0.3)\n",
    "    if abs(detected_aspect - card_specs.aspect_ratio) > aspect_tolerance * card_specs.aspect_ratio:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Check for reasonable card area\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    total_area = frame_width * frame_height\n",
    "    card_area = cv2.contourArea(corners)\n",
    "    \n",
    "    min_area_percentage = 0.02  # Card should occupy at least 2% of the frame\n",
    "    max_area_percentage = 0.9   # Card shouldn't be more than 90% of the frame\n",
    "    \n",
    "    if card_area / total_area < min_area_percentage or card_area / total_area > max_area_percentage:\n",
    "        return False, None, None\n",
    "    \n",
    "    # Create perspective transform to get warped view of the card\n",
    "    # Target size for warped image\n",
    "    target_w = 600\n",
    "    target_h = int(target_w / card_specs.aspect_ratio)\n",
    "    \n",
    "    # Define destination points for perspective transform\n",
    "    dst_pts = np.float32([\n",
    "        [0, 0],\n",
    "        [0, target_h - 1],\n",
    "        [target_w - 1, target_h - 1],\n",
    "        [target_w - 1, 0]\n",
    "    ])\n",
    "    \n",
    "    # Find perspective transform matrix\n",
    "    M = cv2.getPerspectiveTransform(frame_corners.reshape(4, 2).astype(np.float32), dst_pts)\n",
    "    \n",
    "    # Apply perspective transform to get warped image\n",
    "    warped = cv2.warpPerspective(frame, M, (target_w, target_h))\n",
    "    \n",
    "    return True, corners, warped\n",
    "\n",
    "def draw_card(frame, corners):\n",
    "    \"\"\"Draw detected card boundaries\"\"\"\n",
    "    if corners is None:\n",
    "        return frame\n",
    "        \n",
    "    display_frame = frame.copy()\n",
    "    cv2.drawContours(display_frame, [corners], -1, (0, 255, 0), 3)\n",
    "    return display_frame\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Add these variables at the beginning of main()\n",
    "    stable_frames_required = 20     # Number of stable frames required\n",
    "    stable_frame_counter = 0        # Counter for stable frames\n",
    "    last_detection_state = False    # Previous frame detection state\n",
    "    audio_played = False  # Flag to play sound only once per verification cycle\n",
    "    sound_dir = os.path.join(current_dir, \"sounds\")\n",
    "\n",
    "    # Initialize face detector and analyzer\n",
    "    face_detector = init_face_detector()\n",
    "    face_analyzer = init_face_analysis()\n",
    "    \n",
    "    # Initialize ID card detector with reference image - improved error handling\n",
    "    # Replace the single reference image loading with:\n",
    "    # Load reference for student ID card\n",
    "    reference_path_student = os.path.join(current_dir, \"reference_id_card.jpg\")\n",
    "    if not os.path.exists(reference_path_student):\n",
    "        print(f\"Student ID reference image not found at {reference_path_student}\")\n",
    "        return\n",
    "    reference_student = cv2.imread(reference_path_student)\n",
    "    if reference_student is None:\n",
    "        print(\"Could not load student ID card reference image\")\n",
    "        return\n",
    "    card_specs_student = IDCardSpecs.create_standard_id()\n",
    "    reference_features_student = extract_reference_features(reference_student, card_specs_student)\n",
    "    \n",
    "    # Load reference for CCCD (citizen ID card)\n",
    "    reference_path_cccd = os.path.join(current_dir, \"reference_cccd.jpg\")\n",
    "    reference_cccd = None\n",
    "    if os.path.exists(reference_path_cccd):\n",
    "        reference_cccd = cv2.imread(reference_path_cccd)\n",
    "        if reference_cccd is not None:\n",
    "            # Adjust specs for citizen ID card\n",
    "            card_specs_cccd = IDCardSpecs(\n",
    "                aspect_ratio=1.58,  # Adjust if needed\n",
    "                template_features={\n",
    "                    \"photo_area\": (0.10, 0.15, 0.35, 0.70)  # Photo area on CCCD\n",
    "                }\n",
    "            )\n",
    "            reference_features_cccd = extract_reference_features(reference_cccd, card_specs_cccd)\n",
    "        else:\n",
    "            print(\"Could not load CCCD reference image\")\n",
    "    \n",
    "    # Create list of reference cards to detect\n",
    "    references = [\n",
    "        {\"type\": \"Student ID\", \"image\": reference_student, \"features\": reference_features_student, \"specs\": card_specs_student},\n",
    "    ]\n",
    "    \n",
    "    # Add CCCD to references if available\n",
    "    if reference_cccd is not None:\n",
    "        references.append({\n",
    "            \"type\": \"CCCD\", \n",
    "            \"image\": reference_cccd, \n",
    "            \"features\": reference_features_cccd, \n",
    "            \"specs\": card_specs_cccd\n",
    "        })\n",
    "\n",
    "    # Initialize camera - add error handling\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # State variables\n",
    "    state = \"DETECT_CARD\"\n",
    "    card_face_embedding = None\n",
    "    live_face_embedding = None\n",
    "    \n",
    "    print(\"=== Face Verification System ===\")\n",
    "    print(\"Step 1: Detecting ID card automatically\")\n",
    "    sound_path = os.path.join(sound_dir, \"step1.wav\")\n",
    "    sound_play(sound_path)      \n",
    "    # Main loop\n",
    "    running = True\n",
    "    while running:\n",
    "        # Safely capture frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Create display frame to show instructions and results\n",
    "        display = frame.copy()\n",
    "        \n",
    "        # Process key presses first - ensures we always catch the quit key\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            running = False\n",
    "            break\n",
    "        elif key == ord('r') and state == \"SHOW_RESULT\":\n",
    "            state = \"DETECT_CARD\"\n",
    "            card_face_embedding = None\n",
    "            live_face_embedding = None\n",
    "            stable_frame_counter = 0\n",
    "            audio_played = False  # Reset audio flag\n",
    "            print(\"Restarting verification process\")\n",
    "            print(\"Step 1: Detecting ID card automatically\")\n",
    "            sound_path = os.path.join(sound_dir, \"step1.wav\")\n",
    "            sound_play(sound_path)        \n",
    "\n",
    "        # State machine for the verification process\n",
    "        if state == \"DETECT_CARD\":  \n",
    "            card_detected = False\n",
    "            corners = None\n",
    "            warped = None\n",
    "            \n",
    "            # Try to detect each card type\n",
    "            for ref in references:\n",
    "                card_detected, corners, warped = detect_id_card(frame, ref[\"image\"], ref[\"features\"], ref[\"specs\"])\n",
    "                if card_detected:\n",
    "                    detected_specs = ref[\"specs\"]\n",
    "                    detected_type = ref[\"type\"]\n",
    "                    break\n",
    "            \n",
    "            if card_detected:\n",
    "                # Draw card outline\n",
    "                display = draw_card(display, corners)\n",
    "                cv2.putText(display, f\"{detected_type} Detected\", \n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Update frame counter if detection is consistent\n",
    "                stable_frame_counter = stable_frame_counter + 1 if last_detection_state else 0\n",
    "                \n",
    "                # Auto-capture after specified number of stable frames\n",
    "                if stable_frame_counter >= stable_frames_required:\n",
    "                    # Extract face from ID card\n",
    "                    coords = detected_specs.template_features[\"photo_area\"]\n",
    "                    x1, y1, x2, y2 = [int(c * d) for c, d in zip(coords, [warped.shape[1], \n",
    "                                                                warped.shape[0], \n",
    "                                                                warped.shape[1], \n",
    "                                                                warped.shape[0]])]\n",
    "                    card_face_img = warped[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Get face embedding from ID card\n",
    "                    card_face_embedding = get_face_embedding(face_analyzer, card_face_img)\n",
    "                    \n",
    "                    if card_face_embedding is not None:\n",
    "                        state = \"DETECT_FACE\"\n",
    "                        stable_frame_counter = 0\n",
    "                        print(\"Step 2: Detecting live face automatically\")\n",
    "                        sound_path = os.path.join(sound_dir, \"step2.wav\")\n",
    "                        sound_play(sound_path)\n",
    "                    else:\n",
    "                        # Reset counter if face extraction failed\n",
    "                        stable_frame_counter = 0\n",
    "                        cv2.putText(display, \"No face found on card, try again\", \n",
    "                                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(display, \"Move ID card into view\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                stable_frame_counter = 0\n",
    "            \n",
    "            # Update last detection state\n",
    "            last_detection_state = card_detected\n",
    "            \n",
    "        elif state == \"DETECT_FACE\":\n",
    "\n",
    "            faces = detect_faces(face_detector, frame)\n",
    "            \n",
    "            if faces:\n",
    "                # Draw face rectangle\n",
    "                display = draw_face(display, faces)\n",
    "                cv2.putText(display, \"Face Detected\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Update frame counter if detection is consistent\n",
    "                stable_frame_counter = stable_frame_counter + 1 if last_detection_state else 0\n",
    "                \n",
    "                # Auto-capture after specified number of stable frames\n",
    "                if stable_frame_counter >= stable_frames_required:\n",
    "                    # Get face embedding from live face\n",
    "                    live_face_embedding = get_face_embedding(face_analyzer, frame)\n",
    "                    \n",
    "                    if live_face_embedding is not None:\n",
    "                        state = \"SHOW_RESULT\"\n",
    "                        stable_frame_counter = 0\n",
    "                        print(\"Step 3: Comparing faces...\")\n",
    "                    else:\n",
    "                        # Reset counter if face extraction failed\n",
    "                        stable_frame_counter = 0\n",
    "                        cv2.putText(display, \"Face analysis failed, try again\", \n",
    "                                  (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(display, \"Position your face in the camera\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                stable_frame_counter = 0\n",
    "                \n",
    "            # Update last detection state\n",
    "            last_detection_state = len(faces) > 0\n",
    "                \n",
    "        elif state == \"SHOW_RESULT\":\n",
    "            # Compare the face embeddings\n",
    "            if card_face_embedding is not None and live_face_embedding is not None:\n",
    "                similarity = compare_face_embeddings(card_face_embedding, live_face_embedding)\n",
    "                is_match = is_same_person(card_face_embedding, live_face_embedding)\n",
    "                \n",
    "                # Display results - simplified\n",
    "                result_color = (0, 255, 0) if is_match else (0, 0, 255)\n",
    "                result_text = \"MATCH VERIFIED\" if is_match else \"NO MATCH\"\n",
    "                \n",
    "                cv2.putText(display, f\"Result: {result_text}\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, result_color, 2)\n",
    "                cv2.putText(display, f\"Similarity: {similarity:.2f}\", \n",
    "                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                cv2.putText(display, \"Press 'r' to restart, 'q' to quit\", \n",
    "                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                \n",
    "                # Play sound if not already played\n",
    "                if not audio_played:\n",
    "                    sound_file = \"match.wav\" if is_match else \"not_match.wav\"\n",
    "                    sound_path = os.path.join(sound_dir, sound_file)\n",
    "                    sound_play(sound_path)\n",
    "                    audio_played = True\n",
    "            else:\n",
    "                cv2.putText(display, \"Error: Failed to extract face features\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                cv2.putText(display, \"Press 'r' to restart, 'q' to quit\", \n",
    "                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('Face Verification System', display)\n",
    "    \n",
    "    # Clean up properly\n",
    "    print(\"Exiting face verification system\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceverify_id",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
